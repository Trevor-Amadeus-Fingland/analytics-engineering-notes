{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc python commands\n",
    "os.getcwd(): retrieves current working directory in python shell\n",
    "os.listdir(): outputs list of files in directory specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python intro to importing data (data camp)\n",
    "\n",
    "`open(‘file_name,’mode’)`\n",
    "\topens a file in the specified mode\n",
    "\n",
    "`With open(‘file_name’) as name_of_file:`\n",
    "\topens context manager which  allows you to execute commands on a file without needing to explicitly call it each time.\n",
    "\n",
    "`File.readline()`\n",
    "\tMethod to read a single line of a file then iterate to the next line.\n",
    "\n",
    "Flat files are basically weakly formatted tables like csvs or tsvs.\n",
    "\tCan’t do any relational logic on them since they’re simply formatted to look like a table.\n",
    "\n",
    "`Np.loadtxt(file, delimiter = ‘ ’, skiprows = #_of_rows, usecols= [m,n,e], dtype = datatype )`\t\n",
    "\tLoads numeric data as a variable into numpy. \t\tNeed to specify delimiter\n",
    "\tSkiprows skips n number of rows before reading\n",
    "\tusecols only use the columns according to the \n",
    "\tlist of column indices specified.\n",
    "\tNot really meant for mixed data.\n",
    "\n",
    "`Np.genfromtxt(filename,delimiter =‘’,names = True/False dtype = datatype)`\n",
    "\tCan be used to import data into array of mixed types. Done by specifying dtype = None.\n",
    "Names is the header column specification.\n",
    "Retrieve rows with array[n] and columns with array[col_name]\n",
    "Will only be one dimensional arrays, not a 2 dimensional table.\n",
    "\n",
    "`Np.recfromcsv(filename)`\n",
    "\tSimilar to genfromtxt, but automatically assumes dtype none and that the file is a css\n",
    "\n",
    "`pandas_frame.values()`\n",
    "\tConverts pd_frame to a numpy array.\n",
    "\n",
    "`Pd.read_csv(filename, sep = ‘’, comment = ‘’,na_values =)`\n",
    "\tcomment ignores any data following the string \tspecified\n",
    "\tna_values tells what additional data should be  \n",
    "\tconsidered as a null datatype.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special file types\n",
    "**Pickled Files**: Native Python file type that serializes a foreign file type.\n",
    "Below is standard process for importing a pre-pickled file into python.\n",
    "Note that 'rb' indicates read-only and that the file is in a binary format which all pickled files are.\n",
    "```\n",
    "import pickle\n",
    "with open('pickled_file.pkl','rb') as file:\n",
    "    data = pickle.load(file)\n",
    "print(data)\n",
    "```\n",
    "**Excel Sheets**: Example of how to import excel files is below\n",
    "```\n",
    "import pandas as pd\n",
    "file = 'urbanpop.xlsx'\n",
    "data = pd.ExcelFile(file) # ExcelFile is excel file import method.\n",
    "print(data.sheet_names)\n",
    "```\n",
    "Above imports excel file, loads it and prints sheet names.<br>\n",
    "Utilize `data.parse('sheet_name')` to parse the data in particular excel sheet.<br>\n",
    "You can also use `data.parse(n)` where n is the numeric index of the sheets in the excel file. <br>\n",
    "Is ordered from 0 - (m-1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing SAS/Stata Files w Pandas\n",
    "- Most common SAS files are SAS7BDAT(CAT),which stand for data and catalog files respectively.<br><br>\n",
    "Method to import SAS files below:\n",
    "```\n",
    "import pandas as pd\n",
    "from sas7bdat import SAS7BDAT # Have to import a context manager to open sas file\n",
    "with SAS7BDAT('file_name.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "```\n",
    "<br><br>\n",
    "Method to import Stata files:\n",
    "```\n",
    "import pandas as pd\n",
    "data = pd.read_stata('file_name.dta')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing HDF5 files\n",
    "Method to import hdf5 files:\n",
    "```\n",
    "import h5py\n",
    "filename = 'file_name.hdf5'\n",
    "data = h5py.File(filename,'r')\n",
    "print(type(data)) # will report unique hdf5 python class\n",
    "```\n",
    "<br><br>\n",
    "HDF5 class in python can be queried like a dictionary. You can see what keys are in the file with the syntax `for key in data['sub-key'].keys(): print(key)`<br>\n",
    "Sub-key may not be applicable depending on needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing MATLAB files\n",
    "Can be imported with sciPy functions `scipy.io.loadmat()` and `scipy.io.savemat()`. The mat file contains the various objects saved in the matlab file.<br><br>\n",
    "Method to import MATLAB files:\n",
    "```\n",
    "import scipy.io\n",
    "filename = 'file_name.mat'\n",
    "mat = scipy.io.loadmat(filenname)\n",
    "```\n",
    "<br>\n",
    "MATLAB files that are loaded into python are stored as dicts. Key name corresponds to the object/variable name in MATLAB and values are what was actually stored. Key names that correspond to actual matlab variables will not be surrounded be like __name__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relational Databases in Python\n",
    "How to create a database engine in Python(using SQLAlchemy):\n",
    "```\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('db_type:db_name')\n",
    "table_names = engine.table_names() # stores the names of the db tables in a variable as a list\n",
    "```\n",
    "<br><br>\n",
    "How to query a relational database in python\n",
    "```\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_enginen('db_type:///db_name)\n",
    "con = engine.connect() # need to create connection variable\n",
    "query = con.execute(\"Query Logic\") # connection.execute to run sql query\n",
    "df = pd.DataFrame(rs.fetchall()) # retrieves result set from connection and stores to DF\n",
    "df.columns = rs.keys() # applies column names from query to DF that you will manipulate\n",
    "con.close() \n",
    "```\n",
    "<br><br>\n",
    "Using context manager to retrieve query results(removes need to open/close connection):\n",
    "```\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"Query Logic\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=5)) # only fetches specific num of records\n",
    "    df.columns = rs.keys() \n",
    "```\n",
    "<br><br>\n",
    "How to query db directly with Pandas:\n",
    "```\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('db_type:///db_name)\n",
    "df = pd.read_sql_query(\"Query Logic\", engine)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing files from the web\n",
    "Method to get file from web:\n",
    "```\n",
    "from urllib.request import urlretrieve \n",
    "url = 'url_name'\n",
    "urlretrieve(url,'name_for_file')\n",
    "```\n",
    "**urllib** is library to open url files NOT scraping a page. In addition, `urlretrieve` saves the file to a local environment.\n",
    "To retrieve a flat file from the web but load it directly into a DF you can do `pd.read_csv(url,sep = 'separator type')` OR `pd.read_csv(url,'separator type')`.<br>Lastly, this methodology extends to other **Pandas read_file** type functions, for example read_excel.<br><br>\n",
    "Method to read in an xls file from the web:\n",
    "```\n",
    "import pandas as pd\n",
    "url = 'url location'\n",
    "xls = pd.read_excel(url,(sheet_names = [list, of, names] OR None))\n",
    "```\n",
    "None is passed when all excel sheets are desired.<br><br>\n",
    "How to retrieve HTML \n",
    "```\n",
    "from urllib.request import urlopen,Request\n",
    "url = 'url_text'\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()\n",
    "```\n",
    "HTML response is a unique class http.client.HTTPResponse\n",
    "<br><br>\n",
    "How to do the above but with Python requests package:\n",
    "```\n",
    "import requests\n",
    "url = 'url_text'\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "```\n",
    "Note that r.text is NOT a method but an attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping in Python\n",
    "Method to scrape a web page:\n",
    "```\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'url_text'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "```\n",
    "If necessary,BS allows you to reformat a retrieve htmldoc by using `soup.prettify()`<br>\n",
    "In addition, to retrieve the title and text portions of an html doc you can call the attribute and method , `.title` and `get_text()`respectively, of a BS object.<br>\n",
    "BS can also get all the urls from a page with the .find_all() method.<br><br>\n",
    "Retrieving URLs from html_doc with BS:\n",
    "```\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'url_text'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "urls = soup.find_all(a) \n",
    "for link in urls:\n",
    "    print(link.get('href'))\n",
    "```\n",
    "In the soup find_all() method you need to specify what html tag items you want. To get urls you need to specify an html tag of a. The for loop is how to retrieve each link individually and print it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APIS and JSONS Intro\n",
    "Method to extract info from json:\n",
    "```\n",
    "import json\n",
    "with open('file.json','r') as json_file:\n",
    "    json_data= json.load(json_file)\n",
    "```\n",
    "Above opens a json file locally and loads each key-value pair into a dict with `json.load`.\n",
    "<br><br>\n",
    "Method to connect to an API:\n",
    "```\n",
    "import requests\n",
    "url = 'url_api_text'\n",
    "r = requests.get(url)\n",
    "json_data = r.json()\n",
    "for key, value in json_data.items():\n",
    "    print(key+ ':' ,value)\n",
    "```\n",
    "`Dict.items()` outputs a list of tuples with the key value pairs of the dict. The URL api text is going to depend on the api, so make sure to check documentation on the api of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Streaming API and Guide for Data Camp \n",
    "Note that in order to actually use Twitter API you need to create an account to obtain authentication keys in order to actually use python with it. For this course, a mock-up was created to avoid this.<br><br>\n",
    "Method to setup streaming api  and stream in python using tweepy library\n",
    "```\n",
    "import tweepy, json\n",
    "access_token = 'token'\n",
    "access_token_secret = 'secret'\n",
    "consumer_key= 'key'\n",
    "consumer_secret = 'con_secret' \n",
    "\n",
    "stream = tweepy.Stream(consumer_key,consumer_secret,access_token,access_token_secret)\n",
    "stream.filter(track['apples','oranges'])\n",
    "```\n",
    "Note for actual code, probably best to seup a .gitignore file with the sensitive info and pull them in as variables to prevent exposing them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Cleaning Data in Python Course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Type Constraints\n",
    "To retrieve dataframe data types: `DF.dtypes`<br>\n",
    "To get info like # of entries,dtype and nullable status: `DF.info()`<br>\n",
    "Summing string data in Pandas will just concatenate NOT return an error.<br>\n",
    "Categorical data that is a number will be imported as a NUMBER, make sure to convert back to prevent errors.<br><br>\n",
    "Basic process of  cleaning string data in DF with leading/trailing unnecessary character:\n",
    "```\n",
    "DF['column1'] = DF['column1'].str.strip('$')\n",
    "DF['column1'] = DF['column1'].astype('int')\n",
    "```\n",
    "To verify datatype on column use: `assert DF['column'].dtype = 'dtype_to_confirm'`<br>\n",
    "To utilize datetime functions in python import the `datetime` library. The current data can be retrieved using `dt.date.today()`.<br><br>\n",
    "Options to address out of bounds data:<br>\n",
    "- Drop data(should be last option if at all)\n",
    "- Define allowable range\n",
    "- Infer as missing or impute.\n",
    "- Assign custom value based on business understanding of data.<br><br>\n",
    "Methods to remove data in pandas:\n",
    "```\n",
    "df_new = df[df['filter_col'](condition)]\n",
    "df.drop(df[df['filter_col](condition)].index, inplace= True)\n",
    "```\n",
    "<br><br>\n",
    "Methods to constrain data range:\n",
    "```\n",
    "df.loc[df['filter_col'](condition),'column to apply constraints'] = (value to assign)\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "To a pandas object to a date it is necessary to do two-step conversion. The conversion method is: `pd.to_datetime(df['date_col_as_obj']).dt.date`. The first converts a standard pandas object to a pd_datetime object and then second modifies the dtype to the date object in the datetime module. <br>\n",
    "It is possible to just apply dt.date but that requires knowing and inputting the dt format, so the former is more flexible, especially if the underlying dt structure changes.\n",
    "<br><br>\n",
    "Duplicate values can be identified within pandas with `df.duplicated([subset of columns to search],keep)` . `Keep` specifies wheter to keep the first/last/or all instances of a duplicate value.<br>\n",
    "Methods to remove duplicates:\n",
    "```\n",
    "df.drop_duplicates([subset of columns to search(if needed)],keep, inplace)\n",
    "\n",
    "df = df.groupby(by = [columns to group by]).agg(dict of columns and their summaries).reset_index()\n",
    "```\n",
    "The second method is how one might handle duplicates where categorical/string values are duplicated but not the numeric values. Depending on the data and business needs those values can be aggregated using summary metrics. Aggregation in pandas takes a dict as an input of the form: `{'col1':'agg_metric1','col2':'agg_metric2'...}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membership Constraints\n",
    "It is possible to remove errorenous categorical datatypes with either inner or anti-joins(equivalent to set minus operation).<br>\n",
    "Method to remove invalid categories:\n",
    "```\n",
    "bad_categories = set(df1['category_column']).difference(df_lookup['category_column'])\n",
    "bad_rows = df1['category column'].isin(bad_categories)\n",
    "print(df1['bad_rows'])\n",
    "```\n",
    "The above makes the working dataframe column a set to use set operations. Applying difference with the lookup df as a base will reveal only the members unique and invalid in the working dataframe. The bad_rows_df contains booleans on which rows contain the invalid categories. Finally, the bad_rows can be used as a filter to retreive only those rows in our working data.<br>\n",
    "`DF['~col_to_remove']` excludes column data with the tilde next to it but nothing else.<br><br>\n",
    "Method to handle value inconsistencies:\n",
    "```\n",
    "# force casing\n",
    "df['col_with_casing_issue'] = df['col_with_casing_issue'].str.(upper/lower)()\n",
    "# strip leading/trailing spaces\n",
    "df[`col_with_excess_space'] = df['col_with_excess_space].str.strip()\n",
    "```\n",
    "`series.value_counts()` can be used to get counts of a df column or alternatively DF grouping operations can be used instead.<br><br>\n",
    "Method to recategorize or introduce binning:\n",
    "```\n",
    "ranges = [start_val,bound_1,bound_2,... np.inf for ]\n",
    "group_names = ['group_1','group_2',...'group_n']\n",
    "df['category_group'] = pd.cut(df['orig_cats'],bins = ranges,labels = group_names)\n",
    "```\n",
    "<br><br>\n",
    "Method to condense categories:\n",
    "```\n",
    "mapping = {'orig_cat1':'condensed_mapping1','orig_cat2':'condensed_mapping1'...'orig_catn':'condensed_mapping2'}\n",
    "df['categories'] = df['categories].replace(mapping)\n",
    "```\n",
    "The above uses a dict to tell python when replacing what to map the original categories to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Text Data\n",
    "Methods to address text inconsistencies:\n",
    "```\n",
    "# Replace characters\n",
    "df['column_to_adjust'] = df['column_to_adjust'].str.replace(\"char_to_replace\", \"replacement_char\")\n",
    "# Length violations\n",
    "length_series = df['column_to_adjust'].str.len()\n",
    "df.loc[length_series (ineq cond), \"column_to_adjust\"] = np.nan\n",
    "```\n",
    "In order to utilize regex within Pandas after specifying your DF col follow it with `.str.replace(r'regexp_exp','replacement')`. The **r** prior to the regexp expression allows for regexp interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniformity\n",
    "Method to identify numerically measured unit inconsistencies:\n",
    "```\n",
    "series_loc = df.loc[df['column_to_check'](ineq), 'column_to_check']\n",
    "unit_convert = (series_loc with necessary unit conversion ops)\n",
    "df.loc[df['column_to_check'](ineq), 'column_to_check'] = unit_convert\n",
    "```\n",
    "The above locates the indices with the problem units, applies a conversion to remove inconsistent unit measurements and then assigns those indices the converted value.\n",
    "<br><br>\n",
    "Method to identify and correct date measures:\n",
    "```\n",
    "df['date_col']= pd.to_datetime(df['date_col'], infer_datetime_format=True,errors ='coerce')\n",
    "#strftime method\n",
    "df['date_col'] = df['date_col'].dt.strftime(\"dt_format_to_apply\")\n",
    "```\n",
    "The errors argumentin `pd.to_datetime` converts values that cannot be inferred into NaT(NaN for time) vals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Field Validation\n",
    "You can sum across fields in Pandas with: \n",
    "```\n",
    "sum = df[['list','of','fields']].sum(axis = (1/0)) \n",
    "```\n",
    "Note that 0 sums along columns and 1 sums along rows.<br><br>\n",
    "Method to compare sum measure with column data:\n",
    "```\n",
    "bool_filter_series = sum_metric == df['col to validate']\n",
    "invalid_rows = df[~bool_filter_series]\n",
    "valid_rows = df[bool_filter_series]\n",
    "```\n",
    "<br><br>\n",
    "Method for cross-field validation of dt age data:\n",
    "```\n",
    "today = dt.date.today()\n",
    "df['date_val_to_validate'] = pd.to_datetime(df['date_val_to_validate']) # if needed\n",
    "age_validation_col = today.year - df['date_val_to_validate'].dt.year\n",
    "age_match_col = age_validation_col == df['age_val']\n",
    "invalid_age = df[~'age_match_col']\n",
    "valid_age = df['age_match_col']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completeness\n",
    "`missingno` is a good python package for identifying missing data. It is capable of returning vizs for finding missing data.<br><br>\n",
    "Method to generate missing vals plot with missingno:\n",
    "```\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "msno.matrix(DataFrame)\n",
    "plt.show()\n",
    "```\n",
    "It is a good idea to sort your DF before generating the missingno plot as this can help identify patterns in where the data is missing.<br><br>\n",
    "General missing data forms:\n",
    "- **Missing Completely at Random(MCAR)**: Missing data with no relationship to other present or missing data vals.\n",
    "- **Missing at Random(Mar)**:The values of missing data are random but the missing data has some relationship to other observed values in the data set. For example, a doctor failing to ask for smoking status due to patient's gender.\n",
    "-**Missing Not at Random(MNAR)**: Missing data has a relationship with **unobserved** values. This is a difficult type of missing data to assess and correct. An example would be an ER patient not having smoking status determined. It is not random that it wasn't taken because it was due to the emergency situation, but smoking status will definitely impact outcomes in the ER.<br><br>\n",
    "You can replace na values in python with `fillna`. The syntax for it is: `df.fillna((dict w cols as keys and variables/vals as vals))`<br>\n",
    "Na vals can be dropped by column using `dropna` by using the `subset = [list of cols]` argument.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Strings\n",
    "**Minimum edit distance**: Least # of string operations(insertions,deletions,substitutions ,transpositions) to convert word a to word b.\n",
    "The course will use the Levenshtein algorithim in the python `thefuzz` package to assess edit distance. The algorithim uses all operations except transposition.<br><br>\n",
    "Simple example on using `thefuzz` to determine word similarity:\n",
    "```\n",
    "from thefuzz import fuzz\n",
    "fuzz.Wratio('word1','word2')\n",
    "```\n",
    "The Wratio function can understand partials as well as different orderings compared to edit distance.<br><br>\n",
    "Method to extract multiple strings from an object:\n",
    "```\n",
    "from thefuzz import process\n",
    "string = \"string_of_interest\"\n",
    "choices = pd.Series(['list','of','strings','to','extract'])\n",
    "process.extract(string,choices,limit = k)\n",
    "```\n",
    "`Process.extract`will return a list of tuples with the word entry in the object,wratio, and index in the object. The limit arg is the maximum number of words to extract from the object for comparison.<br><br>\n",
    "Method to programatically replace text according to string similarity:\n",
    "```\n",
    "for column in categories['column']:\n",
    "    matches = process.extract(column,df_o_int['column'],limit = df_o_int.shape[0])\n",
    "    for potential_match in matches:\n",
    "        if potental_match[1] >= score_threshold\n",
    "        df_o_int.loc[df_o_int['column'] == potential_match[0], 'state'] = state\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Pairs\n",
    "If there is not a unique key pairs to align two data sources that you need to join, then record linkage is necessary. It is done by creating pairs according to similarity rules. Within python this can be done with the `recordlinkage` package.Since the # of pairs needed can grow quite quickly some identifier , even if it is only a partial match, needs to be used to keep the scale down. This process is called **Blocking**<br><br>\n",
    "Method to generate pairs:\n",
    "```\n",
    "import recordlinkage\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.block('column_to_block_on')\n",
    "pairs = indexer.index(data_a,data_b)\n",
    "```\n",
    "The `indexer` object is the object to generate pairs. Calling `indexer.block()` assigns a column shared between the two datasets to do the blocking on. Lastly, the pair generation is run after inputting the datasets into `indexer.index()` and assigning it to a pairs object. The pairs object is a special multiindex object which has row indicies from both dataframe objects.<br><br>\n",
    "Method to setup rules for recordlinkage.\n",
    "```\n",
    "pairs = indexer.index(census_A,census_B)\n",
    "compare_cl = recordlinkage.Compare()\n",
    "# exact matching\n",
    "compare_cl.exact('col_a_1','col_a_2',label = 'col_a_1&2')\n",
    "# similarity matching\n",
    "compare_cl.string('col_string_1','col_string_2',threshold = 0.xx,label = 'col_string_1&2')\n",
    "# return matches\n",
    "potential_matches = compare_cl.compute(pairs,census_A,census_B)\n",
    "# retrieve rows with only some degree of matching\n",
    "potential_matches[potential_matches.sum(axis = 1) >= t]\n",
    "```\n",
    "The last row on potential matches sums along the rows for each match and determines if the sum of the matches is over a threshold t. This threshold will ultimately be determined by data needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linking DataFrames\n",
    "Method to do record linkage:\n",
    "```\n",
    "# Get indicies from sub-set dataset\n",
    "duplicate_rows = matches.index.get_level_values(1)\n",
    "print(dataset_2_index)\n",
    "dataset_2_duplicates = dataset_2[dataset_2.index.isin(duplicate_rows)]\n",
    "dataset_2_new = dataset_2[~dataset_2.index.isin(duplicate_rows)]\n",
    "full_set = dataset_1.append(dataset_2_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Wide and long data formats\n",
    "Wide data formats have not repeated records and many columns for each record. There may be missing values in these datasets. Ideal for simple statistics and imputing values.<br>\n",
    "By contrast, the narrow(long) data format only has a couple columns per record. Typically, an identfier, variable and value columns. Each record in this case will represents one feature. It is also considered tidy data which is ideal for data summaries and has a key-value format.<br><br>\n",
    "Example to reshape data:\n",
    "```\n",
    "df.set_index('col_to_set_as_index')[['list','of','cols']].transpose()\n",
    "```\n",
    "The above will transpose the dataset with the index as the header row. The indicies will now be the columns before the transposition.<br>\n",
    "Note that `[[col_a,_col_b]]` can be used to only retrieve the specified  columns from a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping using pivot methods\n",
    "`df.pivot(index,columns,values)`: Takes 3 arguments. Index is the column that will be the index, columns is what feature values will appear along the top row and values are what will be populated at the intersection of the rows and columns. Missing values are set to NaN. Inputting a list of features in the `values` argument will create a hierarchical column index. There will be values in the new dataframe corresponding to each feature in the values list. If the `values` argument is omitted it will use all columns in the values argument by default that were not assigned to the index or column. **Note** if there are duplicate values in the unpivoted table the pivot method will fail as it cannot take duplicate entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot Tables\n",
    "`df.pivot()` cannot handle duplicate values for the same index-column pair and is also unable to aggregate data. However, Pandas has another method, pivot tables, that can deal with these.<br><br>\n",
    "`df.pivot_table(index,columns,values,aggfunc,margins)`: The first 3 arguments function identically to those in `df.pivot` but an additional argument `aggfunc` can be specified. Instead of displaying indivdual values for a given index/column pair an aggregation will be performed giving a summarized dataframe. The pivot table can also have hierachical indices in the rows as well as the columns.The last argument, `margins`, creates a grand total style entry that aggregates across the entire row,column and row&column(in bottom left)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping with melt\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

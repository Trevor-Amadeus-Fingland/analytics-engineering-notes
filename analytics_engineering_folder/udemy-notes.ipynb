{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OverView__<br>\n",
    "Analytics Engineering was born out of the shift from ETL(Extract Transform Load) to ELT(Extract Load Transform). <br>\n",
    "General Team makeup and their education\n",
    "- Data Architect: Responsible for working with SMEs(Subject Matter Experts) and devising the blueprints of the data project.\n",
    "- Data Engineers: Build pipelines, infrastructure and data ingest. Typically software engineering background with skill in Java and/or Python.\n",
    "- Analytics Engineer: Responsible for data transformation and modeling so that it can easily be utilized.\n",
    "- BI analyst: Take the transformed data,develop business insights and create a semantic model.\n",
    "- Project Manager(PM): Responsible for project success in a timely manner.\n",
    "<br>\n",
    "Skills that should be developed in order of significance<br>\n",
    "1. SQL,Data Transforms, Data Warehousing and Data Modeling\n",
    "2. Data Orchestration(may be duty of data engineer), BI tools(like tableau), Version Control(github), communication\n",
    "3. Programming,CI/CD,Data Engineering<br>\n",
    "\n",
    "(_personal note_ Likely has something to do with shift from Warehouse to Lakehouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Databases__<br>\n",
    "Databases are managed by DBMS(Database Management System),which is a system that collects metadata on,stores databases and facilitates interaction with databases via SQL or navigation through an interface .Most SQL is ANSI(American National Standards Institute), though not all are, like HiveQL.<br><br>\n",
    "DBMSs were developed to handle various structural considerations that came into play when managing multiple databases, like access control based on user permissions. They can interact and contain more than relational DBs. Other DBs that can be stored are Flat File,Objects and Hiearchical. Common RDMBs are: MS SQL Server, MySQL,Oracle,Postgres,SQLite,MariaDB,AmazonRDS and Google CloudSql. <br><br>\n",
    "Amazon RDS and Google CloudSql are somewhat different in that they are managed database services.\n",
    "These services provide their cloud database to scale your system in accordance with demand and obviate the need for maintenance of database system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sheets v Databases__<br>\n",
    "Databases cannot be modified like spreadsheets by simply editing a cell. It is necessary to use SQL via INSERT,DELETE,CREATE statements.<br><br>\n",
    "Differences between Spreadsheet and Database\n",
    "|SpreadSheet                                  | Database  |\n",
    "|---------------------------------------------|-----------|\n",
    "|Size limitation, approx 1M rows and 16k Cols | Size only limited by cost and resources|\n",
    "|Entire data set loaded                       | Dynamic i.e. only collects/changes what is queried|\n",
    "|Static unless macros/formulas are used |Dynamic, a single change will get propogated everywhere it's used|\n",
    "|Simple creation process                      | Can be complex dependent upon requirements|\n",
    "|Does not scale well,usually more error prone than a database,and metadata is either non-existent or poor| SQL operates the same regardless of table scale |\n",
    "\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OLTP__<br>\n",
    "Databases can be broken into two major types, SQL and NO-SQL(means NOT Only sql).NO-SQL is capable of dealing with semi-structured(JSONs/mappings) or unstructured(open-ended comments on a forum) data. It can also access structured data as well.<br><br>\n",
    "Relational OLTPs(OnLine Transaction Processing) is used in transactional data. Examples include CRM or banking data. This kind of processing excels in running insert, update and/or delete operations. Data is stored as rows and must be available 24/7.Due to the always online nature of the systems that use OLTP they also have substantial disaster recovery and backup systems.<br><br>\n",
    "The OLTP source is the original source of data and typically acts as a source of truth.Queries tend to be simplistic and usually only act on a few records at a time. An example is applying an update to a single customer record after they updated their phone number. Data is highly normalized with minimal data redundancies. <br><br>\n",
    "In brief, normalization is the process by which relationships are created between tables. The result is that redundancies are reduced, removes inconsistent data dependencies and preserves data integrity. An example of this process would breaking out a single sales table into smaller sub-tables. One table would be a lookup for customer details, the other details items in each individual order, and the last being a high-level overview of orders places. These can then be connected back to each other with a customer id or order id depending on what tables need to be joined for analysis.<br><br>\n",
    "Summary\n",
    "- OLTPs tend to utilize normalized tables\n",
    "- Multiple joins typically needdueto normalization\n",
    "- Schema and data types are defined ahead of time\n",
    "- Not good for reporting\n",
    "- OLTPs are ACID compliant\n",
    "- Optimized for collecting rather than aggregating data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OLAP__\n",
    "Stands for Online Analytical Processing. They are used for analytical queries and as such are the workhorse for BI. Data is organized column by column rather than by row and column. Allows for data compression and faster data processing. In addition, searches to retrieve data do not need to go row by row, but can scan through columns instead.<br><br>\n",
    "Data is composed of OLTP databases and other third party data sources. Tables in OLAP tend to have less tables but are denormalized. Join minimization increases query performance. Examples of OLAP systems are Snowflake, Google Big Query, Firebolt and PostgresSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NO-SQL__\n",
    "These are non-relational databases and are usually used in tandem with relational databases. No-SQL databases come in a few flavors.\n",
    "- Key-Value stores: these are mappings where a unique key is called to retrieve data from the database. These are useful when data retrieval speed is important. Queries and data tend to not be complex. These are used in things like shopping carts, user sessions or  product recommendations.\n",
    "- Document Stores: This database stores records and data as a single document. There is no need for schema creation and scales well horizontally via sharding. These documents are JSON-like. Multiple documents are referred to as collections. They are good for product catalogs, web apps and e-commerce.\n",
    "- Wide Column stores: Stores data in columns instead of rows. The difference from OLAP is that the format of the columnns doesn't need to be consistent between columns. These systems are highly scalable. Columns only need not exist in every row like in OLAP. However, this functionality limits usage of the store. It shouldn't be used for ad-hoc or highly aggregated data sets. Used for doing real time analytics and timeseries data.\n",
    "- Graph Database: A database structured around relationships between entities. Entities are stored as nnodes and relationships are stored as edges. Information associated with nodes are called properties. Being centered on relationships allows for quick query access as they do not need to be calculated at query execution. This also allows for complex querying that will be independent of the data size. These are used in ML tools like Recommenders or NLPs.They can also be used for fraud detection, social networks or logistics.\n",
    "- Search Engine Databases: These are databases that allow for data retrieval via search tool. This could be a web search or full text search. Data is stored in a JSON document with no schema. Example use cases include full-text searchs, time series data, and auto suggestion/auto completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On-Prem v Cloud__<br>\n",
    "On prem means that the database is running and installed within the orgainization. This requires engineers to maintain the systems. There may be a need for down time for upgrades and there is also a higher upfront cost due to the need to pay for all the tech and hardware.<br><br>\n",
    "Cloud is where your data infrastructure is stored remotely by a third party orgainization. Examples are Google, Microsoft and Amazon. They take care of the cost of maintaining the infrastructure. There are also generous SLAs and you will be compensated if any of their systems fail. You only pay for what you use so this avoids upfront costs. It is also allows for near instant provisioning of resources like VMs as it is pre-configured by the servicer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Warehouse Def__<br>\n",
    "Not the same as an OLTP database, but stores a copy of the OLTP source so it can be used. Also stores data from other sources, analytics events,IOT, metadata etc.. Designed for analytic usage. These are good for big data and data is structured according to the dimensional model. ACID is not as important for a data warehouse. Examples are Google BigQuery, Redshift and Snowflake.<br><br>\n",
    "Data Warehouse examples\n",
    "- Social Media Platform: use the warehouse to gather user data and drives insights for ad campaigns.\n",
    "- Banking: Identify spending patterns and identify fraud.\n",
    "- Governments: Detect tax fraud by using a data warehouse to analyze tax payment records.\n",
    "\n",
    "<br><br>\n",
    "Data Warehouse Characteristics\n",
    "- Subject Oriented: Focused on a subject like a customer, sales or marketing. Not interested in day to day operations. Provides concise view of subject. Optimized for analytics questions and superfluous data is excluded.\n",
    "- Integrated: This means that the data is retrieved from multiple sources and then standardize according to defined rules.\n",
    "- Time-Variant: Data is organized by time intervals, could be days,years or weeks. Allows for historical analysis and prevents changes to historical data.\n",
    "- Non-volatile: Data entered without erasure of prior data. Data is read only with only options for loading and accessing data. Data refreshes and modification operations like Delete/Update/Insert are omitted.<br><br>\n",
    "\n",
    "Benefits:\n",
    "- Historical data insights\n",
    "- Improves quality and helps establish data lineage\n",
    "- Increase Effciency\n",
    "- Fast Query results\n",
    "- Provides BI functionality\n",
    "- High reliability and scalability\n",
    "- Connects on-site and cloud infrastructure\n",
    "- Enhanced data security\n",
    "- Improved revenue\n",
    "- Provides a competitive advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data DW Architecture__\n",
    "There are three types of data warehouse architecture: single,two and three tier.<br><br>\n",
    "SINGLE:<br>\n",
    "All source data lives inside the data warehouse layer.It saves onn storage and redundancies. However, this is not ideal for lots of data streams or complex enterprise requirements. It also means your BI tools communicate directly with the Warehouse Layer which can cause reduced performance.<br><br>\n",
    "TWO-TIER:\n",
    "Data sources a removed being stored directly in the data warehouse. Instead, it is stored in a data lake outside the warehouse layer. This eliminates of raw data living within your warehouse. There is also a staging layer where data transformation occurs. It improves data lineage and governance as a result.<br><br>\n",
    "THREE TIER:\n",
    "The three tier adds a data mart layer after the warehouse layer. BI tools do not query the warehouse layer in this form, but rather a data mart layer. Data marts contain smaller, domain-specific data that is typically under 100 gb. This improves performance on the BI tools as they do not need to navigate a massive data warehouse layer.<br><br>\n",
    "NOTE: It should be noted that the descriptions of these structures don't reflect actual business data architecture. Rather a generalized form that can be used to understand actual data architectures.<br><br>\n",
    "When referring to components of a three tier architecture there are bottom/middle and top tiers. The bottom tier involves the raw data, data lake and staging layer where raw data is transformed. The middle tier is where the warehouse and data mart layers live and is also where data modeling occurs. Lastly, the top tier refers to the BI tools that operate on the data mart layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Source layer__<br>\n",
    "Data Sources can be anything where data is generated. This could be telemetry from sensors, manually entered information into an excel sheet, internal tools and/or a copy of your OLTP data. As the data representation is a varied as the possible sources, the data is far from clean at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Lake__<br>\n",
    "Data lakes are where all the data from your disparate sources is first stored. This removes the need to refer to your raw data sources. Although, at this stage the data is still raw but at least is now all under one roof. Data within the lake is usually stored in folders or sub-folders with some naming convention. They are also time stamped. Just about all data is stored in a data lake due to the very low cost. Data pruning happens when moving data into a warehouse layer.<br><br>\n",
    "5 V's of Data:<br>\n",
    "- Volume: How much raw data is there? Exobytes, terabytes etc..\n",
    "- Velocity: How quickly is data being transferred into your system or being processed. Includes batch and real time data. It becomes important to understand how to capture and effectivley store this data.\n",
    "- Variety: What are your data sources and what kind of data structure is it? Larger variety means there is more work to standardize and transform the data.\n",
    "- Veracity: How much can the data be trusted? Data quality assertions are necessary to ensure your data is trustworthy when arriving at the data warehouse layer.\n",
    "- Value: How much actual value is within the data stored. This is directly dependent on veracity as untrustworthy data has very little value.<br><br>\n",
    "Why data lakes?<br>\n",
    "Highly scalable, flexible and allows for convenient storage as files. They also provide data management services and remove the risk of data siloing. Because of the nature of a data lake you can store essentially any type of data even if it isn't relational or transactional. Because all data gets stored in the lake there's no concern with needing to shuffle the data around. Data lakes are also good for AI and ML since raw data tends to be best for those technologies.<br><br>\n",
    "Data Lake Storage Options:<br>\n",
    "G-Cloud<br>\n",
    "- Standard(aka hot data): This is a data storage option for regularly stored data.\n",
    "- Nearline: This is data stored for at least 30 days and typically where backup data is stored. This is a cheaper storage option.\n",
    "- Coldline: Data that is irregularly used(>90 days). This storage option is meant for doing data health checks and disaster recovery.\n",
    "- Archive: For data that isn't used for at least a year. In particular this is where you might store data for regular yearly audits. This is the cheapest option.<br><br>\n",
    "AWS S3(only options not in G-cloud are listed):<br>\n",
    "- Intelligent Tiering: Automated data storage option that infers how the data should be stored according to usage patterns.\n",
    "- Standard/One Zone -IA: Used for data that  is touched infrequently, usually after 30 days.\n",
    "- Glacier: Used for archiving data for up to 90 days.\n",
    "- Glacier Deep Archive: Used for archiving data for up to 180 days.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Warehouse Layer__\n",
    "The first layer in the data warehouse layer is the `staging layer`. This layer sits between the data lake and the layer the data eventually gets sent to. Here, data is aggregated, cleaned and standardized. These are extracted through ELT or ETL tools. The data is still raw though and hasn't been aggregated in an analytic sense.<br><br>\n",
    "\n",
    "The next layer is the `data warehouse layer` proper. Additional data transformation and formatting occurs here. This is where data modelling is applied and also where data is transformed in such a way that it no longer raw data. Removal, masking or hashing of PII occurs at this stage. It is also where data access permissions are assigned. Data might be aggregated here to improve access optimizationn. Lastly,metadata and data lineage are created within this layer.\n",
    "\n",
    "The third layer within the warehouse layer is the `data mart layer`. The definition of a data mart is not wholly consistent. Within this course series though the creator considers it a sub-set of the data warehouse. Here data is reduced in size to only contain data relevant to business processes or certain departments. This allows for easier security measures and easier to manage. For example, if a business process involves manipulating data for a third party partner then you would want to restrict that to a data mart for the appropriate team. This is also the layer used by BI tools and processes. Data is also summarized/aggregated here for business uses whereas, aggregation in the warehouse layer either does not occur or only done if there are issues with performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Business Intelligence__<br>\n",
    "This is the part of data architecture in the the top tier. These can be either technologies, tools or strategies. They are utilized to perform analysis on historical or recent data. It is also the end non-data professionals use to identify busniess insights. They help to give an overall view of whatever your end goal is be it product or customer and allows for data-driven decision making. These tool provide for vizs like KPI reports or ML model outputs. These can also be used to track live events like a new product launch or inform PMs and Stakeholders on their customer base. Traditionally BI has been a kind of static or batch update. In more modern times, BI now allows for interactive and self-service design so stakeholders cna retrieve the information they need as it is needed. In addition, these tools also have data enrichment capabilities as well such as further aggregation and custom calculations.<br><br>\n",
    "\n",
    "BI TOOLS\n",
    "- Looker: API data platform that sits on top of databases. Has LookML language for data modelling, centrally located definitions, metrics, and governance, and caching. There is also git versioning as well. Cloud native service as well.\n",
    "- Data Studio: Google cloud platform tool. Is an easy and simple to use(drag&drop) google BI tool that is also cloud native. Has real-time dashboard collaboration and integrated directly with Google Big Query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ELT vs ETL__<br>\n",
    "- Extract: First step in either process. Here the data is pulled from the raw sources and placed either in temp or perm storage. From there a decision is made on which data should be batch(freq too) and which should be real-time. Lastly, validation checks are run. These checks include things like Null checks and data formatting. The validation steps are not so much quality control as they are keeping track of changes to how the raw data is coming in.\n",
    "- Transform: This is the process of cleaning or enriching the raw data. `Cleansing` is the process by which data is checked for data inconsistencies. So any issues within the data are flagged and then removed. From there, the data is `standardized`. This is where a uniform format is applied to the raw data. `Deduping` is where redundant data is removed. Next, there is the `verification` step where flagging of anomalous or unusable data(incorrect data type) occurs. `Sorting` occurs next which is where the data is reorganized according to organizational needs. Finally, there may be additional Transform tasks that needed to be done like re-mapping raw data names to a standardized form from a lookup table or more complex data transforms like unloading unstructured data(JSON) into individual columns.\n",
    "- Load: This is where the now transformed data is moved into the data warehouse layer. Data QA checks occur here and recovery mechanisms are devised in the case of failure. Loading type is also determined. It can either be `Full refresh` or `Incremental` where data is entirely reloaded on each interval or only for a certain swathe of time respectively.\n",
    "\n",
    "ETL assignment and details according to standard architecture defined in previous lecture:<br>\n",
    "- Raw to staging data loading iis where `extraction` occurs \n",
    "- Within the staging table is where `transformation` occurs.\n",
    "- `Loading` occurs when the staging data is successfully transformed and moved to the data warehouse layer.\n",
    "- In this methodology, E&T are mostly performed by data engineers with some assistance from analytics engineers or data warehouse developers. After loading Analytics Engineers do additional processing on the data so that BI specialists and Data Scientists can begin using it.\n",
    "- Advantages include:\n",
    "    - Complex transforms before loading into warehouse\n",
    "    - Only relevant or required data is loaded into the warehouse\n",
    "    - Data not needed remains in the lower cost tier\n",
    "    - Removes or encrypt PII or sensitive data before it gets to the warehouse layer\n",
    "    - Allows for fast analysis.\n",
    "- Disadvantages include:\n",
    "    - Requires heavy involvement of data engineers to implement\n",
    "    - Flow and transformation before getting to warehouse is complex which could cause a bottleneck\n",
    "    - Extracting only necessary data could reduce potential insights or discoveries since other data is excluded prior\n",
    "    - Introduces back and forth as data needs change for BI professionals which requires them to request that data rather than retrieving it themselves.\n",
    "    - Self-service not as achievable due to the back-and-forth nature\n",
    "    - Maintenance requires precision and it must be a well defined orchestration.\n",
    "\n",
    "ELT assignment according to standard architecture defined in previous lecture:<br>\n",
    "- `Extract` and `load` occurs between the raw source data and the data lake. So all of the source data sits in the lake. Next the data is loaded in to the staging layer which differs from the ETL process.\n",
    "- In this method data engineers can handle both E&T without help from other data professionals.\n",
    "- The `transform` stage is then handled in varying degrees by the data warehouse developers,analytics engineers and BI professionals.\n",
    "- Data is then made available through data marts or other modelling options where BI analysts can connect to the data.\n",
    "- Advantages Include:\n",
    "    - More flexible storage since all data is pulled into the lake\n",
    "    - Enables analysis on a wider variety of data since it is not pruned before getting to the warehouse layer\n",
    "    - Future proofs architecture as all data is loaded into the lake, so engineering changes do not need to be made to pull in additional data. The lake is also resilient to increases in volume and can readily scale.\n",
    "    - Removes the need for data engineers to develop complicated pipelines\n",
    "    - Less maintenace needed since cloud solutions are usually involved here.\n",
    "    - Data security can now be managed within the data warehouse.\n",
    "- Disadvantages Include:\n",
    "        - More storage space needed, not as much of a concern if you're using a scalable cloud solution.\n",
    "        - Can lead to slow performance if ELT pipeline is setup poorly.\n",
    "            - Could also be slower due to a large volume of data existing within multiple layers\n",
    "        - Compliance and security harder to achieve due all data now existing in the data lake.\n",
    "            - Possible to remedy this with careful design processes and strong data governance.\n",
    "        - Newer method coming out of cloud solutions, so harder to come by well-informed talent or high quality resources.\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

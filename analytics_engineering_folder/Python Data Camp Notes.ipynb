{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc python commands\n",
    "os.getcwd(): retrieves current working directory in python shell\n",
    "os.listdir(): outputs list of files in directory specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python intro to importing data (data camp)\n",
    "\n",
    "`open(‘file_name,’mode’)`\n",
    "\topens a file in the specified mode\n",
    "\n",
    "`With open(‘file_name’) as name_of_file:`\n",
    "\topens context manager which  allows you to execute commands on a file without needing to explicitly call it each time.\n",
    "\n",
    "`File.readline()`\n",
    "\tMethod to read a single line of a file then iterate to the next line.\n",
    "\n",
    "Flat files are basically weakly formatted tables like csvs or tsvs.\n",
    "\tCan’t do any relational logic on them since they’re simply formatted to look like a table.\n",
    "\n",
    "`Np.loadtxt(file, delimiter = ‘ ’, skiprows = #_of_rows, usecols= [m,n,e], dtype = datatype )`\t\n",
    "\tLoads numeric data as a variable into numpy. \t\tNeed to specify delimiter\n",
    "\tSkiprows skips n number of rows before reading\n",
    "\tusecols only use the columns according to the \n",
    "\tlist of column indices specified.\n",
    "\tNot really meant for mixed data.\n",
    "\n",
    "`Np.genfromtxt(filename,delimiter =‘’,names = True/False dtype = datatype)`\n",
    "\tCan be used to import data into array of mixed types. Done by specifying dtype = None.\n",
    "Names is the header column specification.\n",
    "Retrieve rows with array[n] and columns with array[col_name]\n",
    "Will only be one dimensional arrays, not a 2 dimensional table.\n",
    "\n",
    "`Np.recfromcsv(filename)`\n",
    "\tSimilar to genfromtxt, but automatically assumes dtype none and that the file is a css\n",
    "\n",
    "`pandas_frame.values()`\n",
    "\tConverts pd_frame to a numpy array.\n",
    "\n",
    "`Pd.read_csv(filename, sep = ‘’, comment = ‘’,na_values =)`\n",
    "\tcomment ignores any data following the string \tspecified\n",
    "\tna_values tells what additional data should be  \n",
    "\tconsidered as a null datatype.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special file types\n",
    "**Pickled Files**: Native Python file type that serializes a foreign file type.\n",
    "Below is standard process for importing a pre-pickled file into python.\n",
    "Note that 'rb' indicates read-only and that the file is in a binary format which all pickled files are.\n",
    "```\n",
    "import pickle\n",
    "with open('pickled_file.pkl','rb') as file:\n",
    "    data = pickle.load(file)\n",
    "print(data)\n",
    "```\n",
    "**Excel Sheets**: Example of how to import excel files is below\n",
    "```\n",
    "import pandas as pd\n",
    "file = 'urbanpop.xlsx'\n",
    "data = pd.ExcelFile(file) # ExcelFile is excel file import method.\n",
    "print(data.sheet_names)\n",
    "```\n",
    "Above imports excel file, loads it and prints sheet names.<br>\n",
    "Utilize `data.parse('sheet_name')` to parse the data in particular excel sheet.<br>\n",
    "You can also use `data.parse(n)` where n is the numeric index of the sheets in the excel file. <br>\n",
    "Is ordered from 0 - (m-1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing SAS/Stata Files w Pandas\n",
    "- Most common SAS files are SAS7BDAT(CAT),which stand for data and catalog files respectively.<br><br>\n",
    "Method to import SAS files below:\n",
    "```\n",
    "import pandas as pd\n",
    "from sas7bdat import SAS7BDAT # Have to import a context manager to open sas file\n",
    "with SAS7BDAT('file_name.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "```\n",
    "<br><br>\n",
    "Method to import Stata files:\n",
    "```\n",
    "import pandas as pd\n",
    "data = pd.read_stata('file_name.dta')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing HDF5 files\n",
    "Method to import hdf5 files:\n",
    "```\n",
    "import h5py\n",
    "filename = 'file_name.hdf5'\n",
    "data = h5py.File(filename,'r')\n",
    "print(type(data)) # will report unique hdf5 python class\n",
    "```\n",
    "<br><br>\n",
    "HDF5 class in python can be queried like a dictionary. You can see what keys are in the file with the syntax `for key in data['sub-key'].keys(): print(key)`<br>\n",
    "Sub-key may not be applicable depending on needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing MATLAB files\n",
    "Can be imported with sciPy functions `scipy.io.loadmat()` and `scipy.io.savemat()`. The mat file contains the various objects saved in the matlab file.<br><br>\n",
    "Method to import MATLAB files:\n",
    "```\n",
    "import scipy.io\n",
    "filename = 'file_name.mat'\n",
    "mat = scipy.io.loadmat(filenname)\n",
    "```\n",
    "<br>\n",
    "MATLAB files that are loaded into python are stored as dicts. Key name corresponds to the object/variable name in MATLAB and values are what was actually stored. Key names that correspond to actual matlab variables will not be surrounded be like __name__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relational Databases in Python\n",
    "How to create a database engine in Python(using SQLAlchemy):\n",
    "```\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('db_type:db_name')\n",
    "table_names = engine.table_names() # stores the names of the db tables in a variable as a list\n",
    "```\n",
    "<br><br>\n",
    "How to query a relational database in python\n",
    "```\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_enginen('db_type:///db_name)\n",
    "con = engine.connect() # need to create connection variable\n",
    "query = con.execute(\"Query Logic\") # connection.execute to run sql query\n",
    "df = pd.DataFrame(rs.fetchall()) # retrieves result set from connection and stores to DF\n",
    "df.columns = rs.keys() # applies column names from query to DF that you will manipulate\n",
    "con.close() \n",
    "```\n",
    "<br><br>\n",
    "Using context manager to retrieve query results(removes need to open/close connection):\n",
    "```\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"Query Logic\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=5)) # only fetches specific num of records\n",
    "    df.columns = rs.keys() \n",
    "```\n",
    "<br><br>\n",
    "How to query db directly with Pandas:\n",
    "```\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('db_type:///db_name)\n",
    "df = pd.read_sql_query(\"Query Logic\", engine)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing files from the web\n",
    "Method to get file from web:\n",
    "```\n",
    "from urllib.request import urlretrieve \n",
    "url = 'url_name'\n",
    "urlretrieve(url,'name_for_file')\n",
    "```\n",
    "**urllib** is library to open url files NOT scraping a page. In addition, `urlretrieve` saves the file to a local environment.\n",
    "To retrieve a flat file from the web but load it directly into a DF you can do `pd.read_csv(url,sep = 'separator type')` OR `pd.read_csv(url,'separator type')`.<br>Lastly, this methodology extends to other **Pandas read_file** type functions, for example read_excel.<br><br>\n",
    "Method to read in an xls file from the web:\n",
    "```\n",
    "import pandas as pd\n",
    "url = 'url location'\n",
    "xls = pd.read_excel(url,(sheet_names = [list, of, names] OR None))\n",
    "```\n",
    "None is passed when all excel sheets are desired.<br><br>\n",
    "How to retrieve HTML \n",
    "```\n",
    "from urllib.request import urlopen,Request\n",
    "url = 'url_text'\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()\n",
    "```\n",
    "HTML response is a unique class http.client.HTTPResponse\n",
    "<br><br>\n",
    "How to do the above but with Python requests package:\n",
    "```\n",
    "import requests\n",
    "url = 'url_text'\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "```\n",
    "Note that r.text is NOT a method but an attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping in Python\n",
    "Method to scrape a web page:\n",
    "```\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'url_text'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "```\n",
    "If necessary,BS allows you to reformat a retrieve htmldoc by using `soup.prettify()`<br>\n",
    "In addition, to retrieve the title and text portions of an html doc you can call the attribute and method , `.title` and `get_text()`respectively, of a BS object.<br>\n",
    "BS can also get all the urls from a page with the .find_all() method.<br><br>\n",
    "Retrieving URLs from html_doc with BS:\n",
    "```\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'url_text'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "urls = soup.find_all(a) \n",
    "for link in urls:\n",
    "    print(link.get('href'))\n",
    "```\n",
    "In the soup find_all() method you need to specify what html tag items you want. To get urls you need to specify an html tag of a. The for loop is how to retrieve each link individually and print it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APIS and JSONS Intro\n",
    "Method to extract info from json:\n",
    "```\n",
    "import json\n",
    "with open('file.json','r') as json_file:\n",
    "    json_data= json.load(json_file)\n",
    "```\n",
    "Above opens a json file locally and loads each key-value pair into a dict with `json.load`.\n",
    "<br><br>\n",
    "Method to connect to an API:\n",
    "```\n",
    "import requests\n",
    "url = 'url_api_text'\n",
    "r = requests.get(url)\n",
    "json_data = r.json()\n",
    "for key, value in json_data.items():\n",
    "    print(key+ ':' ,value)\n",
    "```\n",
    "`Dict.items()` outputs a list of tuples with the key value pairs of the dict. The URL api text is going to depend on the api, so make sure to check documentation on the api of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Streaming API and Guide for Data Camp \n",
    "Note that in order to actually use Twitter API you need to create an account to obtain authentication keys in order to actually use python with it. For this course, a mock-up was created to avoid this.<br><br>\n",
    "Method to setup streaming api  and stream in python using tweepy library\n",
    "```\n",
    "import tweepy, json\n",
    "access_token = 'token'\n",
    "access_token_secret = 'secret'\n",
    "consumer_key= 'key'\n",
    "consumer_secret = 'con_secret' \n",
    "\n",
    "stream = tweepy.Stream(consumer_key,consumer_secret,access_token,access_token_secret)\n",
    "stream.filter(track['apples','oranges'])\n",
    "```\n",
    "Note for actual code, probably best to seup a .gitignore file with the sensitive info and pull them in as variables to prevent exposing them.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
